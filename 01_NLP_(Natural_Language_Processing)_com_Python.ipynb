{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-y5DJEydnkDo"
      },
      "source": [
        "Discutiremos uma visão geral dos fundamentos do Processamento de Linguagem Natural, que consiste em combinar técnicas de aprendizado de máquina com texto, utilizando matemática/estatística para obter esse texto em um formato que os algoritmos de aprendizado de máquina possam entender.\n",
        "\n",
        "    \n",
        "**Requisitos: Você precisará ter o NLTK instalado, juntamente com o download do corpus para palavras irrelevantes (stopwords)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tUbx5qxtnkDs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjeGQESqnkDt"
      },
      "source": [
        "## Obtenha os dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSPYZuJJnkDu"
      },
      "source": [
        "Usaremos um conjunto de dados do [UCI datasets](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection). Este conjunto de dados já está localizado na pasta desta seção."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGR_nRJsnkDv"
      },
      "source": [
        "O arquivo que estamos usando contém uma coleção de mais de 5 mil mensagens telefônicas SMS. Você pode conferir o arquivo **readme** para mais informações.\n",
        "\n",
        "Vamos em frente e usar rstrip() mais uma compreensão de lista para obter uma lista de todas as linhas de mensagens de texto:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Esse trecho do código é obrigatório para quem estiver fazendo tudo pelo colab\n",
        "# Caso você esteja utilizando o jupyter pode comentar/apagar\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"drive/My Drive/Colab Notebooks/IA/20_Natural_Language_Processing\")\n",
        "os.listdir()"
      ],
      "metadata": {
        "id": "_zmwvyy9pRAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INMKMzI0nkDv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2MTwzHJnkDx"
      },
      "source": [
        "Uma coleção de textos também é chamada de \"corpus\". Vamos imprimir as dez primeiras mensagens e numerá-las usando **enumerate**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjNmPsEInkDx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIyZb4dznkDy"
      },
      "source": [
        "Devido ao espaçamento, podemos dizer que este é um [TSV](http://en.wikipedia.org/wiki/Tab-separated_values) (\"tab separated values\") arquivo, onde a primeira coluna é um rótulo dizendo se a mensagem fornecida é uma mensagem normal (comumente conhecida como \"ham\") ou \"spam\". A segunda coluna é a própria mensagem. (Observe que nossos números não fazem parte do arquivo, eles são apenas da chamada **enumerate**).\n",
        "\n",
        "Usando esses exemplos rotulados de ham e spam, **treinaremos um modelo de aprendizado de máquina para aprender a discriminar entre ham/spam automaticamente**. Então, com um modelo treinado, poderemos **classificar mensagens arbitrárias sem rótulo** como ham ou spam."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_LbFIoRnkD0"
      },
      "source": [
        "Em vez de analisar o TSV manualmente usando Python, podemos aproveitar os pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "VwYf09BEnkD0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xNW718unkD1"
      },
      "source": [
        "Usaremos read_csv com o argumento sep, também podemos especificar os nomes das colunas desejadas passando uma lista de nomes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aO-jCgrZnkD1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1Q95Bo6nkD1"
      },
      "source": [
        "## Análise exploratória de dados\n",
        "\n",
        "Vamos conferir algumas das estatísticas com gráficos e os métodos embutidos no pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SIpw2h7nkD2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_az6EsLUnkD2"
      },
      "source": [
        "Vamos usar **groupby** para usar descrever por rótulo (label), dessa forma podemos começar a pensar nas características que separam ham e spam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmxJmcoGnkD2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgFFG8zjnkD2"
      },
      "source": [
        "À medida que continuamos nossa análise, queremos começar a pensar nas características que usaremos. Isso está de acordo com a ideia geral de [feature engineering](https://en.wikipedia.org/wiki/Feature_engineering). Quanto melhor seu conhecimento de domínio sobre os dados, melhor sua capacidade de projetar mais carcterísticas a partir deles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NpXDVw9nkD3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjgUBX0vnkD3"
      },
      "source": [
        "### Visualização de dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9Neg5gXnkD4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kmic88OvnkD5"
      },
      "source": [
        "Parece que o comprimento do texto pode ser uma boa característica para se pensar! \n",
        "\n",
        "Vamos tentar explicar por que o eixo x vai até 1000, isso deve significar que há uma mensagem muito longa!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7W8c5nPGnkD6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sB4k2RhRnkD6"
      },
      "source": [
        "910 caracteres! vamos encontrar esta mensagem:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWQd4bs4nkD6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojS2-5Z4nkD6"
      },
      "source": [
        "Parece que temos algum tipo de Romeu mandando mensagens!\n",
        "\n",
        "Mas vamos nos concentrar novamente na ideia de tentar descobrir se o comprimento da mensagem é uma característica distintiva entre ham e spam:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wRSwi45nkD6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TAmAI26nkD7"
      },
      "source": [
        "Por meio de um EDA básico, conseguimos descobrir uma tendência de que as mensagens de spam tendem a ter mais caracteres. (Desculpe Romeu!)\n",
        "\n",
        "Agora vamos começar a processar os dados para que possamos usá-los com o SciKit Learn!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geVin6qZnkD7"
      },
      "source": [
        "## Pré-processamento de texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNRaomslnkD7"
      },
      "source": [
        "Nosso principal problema com nossos dados é que estão todos em formato de texto (strings). Os algoritmos de classificação que aprendemos até agora precisarão de algum tipo de vetor de recursos numéricos para realizar a tarefa de classificação. Na verdade, existem muitos métodos para converter um corpus em um formato vetorial. O mais simples é a abordagem de [bag-of-words](http://en.wikipedia.org/wiki/Bag-of-words_model), onde cada palavra única em um texto será representada por um número.\n",
        "\n",
        "Vamos converter as mensagens brutas (sequência de caracteres) em vetores (sequência de números).\n",
        "\n",
        "Primeiro, vamos escrever uma função que dividirá uma mensagem em suas palavras individuais e retornará uma lista. Também removeremos palavras muito comuns ('the', 'a', etc.). Para isso, vamos aproveitar a biblioteca **NLTK**. Ela é praticamente a biblioteca padrão em Python para processamento de texto e possui muitos recursos úteis. Usaremos apenas alguns dos básicos aqui.\n",
        "\n",
        "Vamos criar uma função que irá processar a string na coluna de mensagem, então podemos usar **apply()** do pandas para processar todo o texto no DataFrame.\n",
        "\n",
        "Primeiro removendo a pontuação. Podemos apenas aproveitar a biblioteca **string** incorporada do Python para obter uma lista rápida de todas as pontuações possíveis:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3m2w1riMnkD7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GftDFzcUnkD7"
      },
      "source": [
        "Agora vamos ver como remover palavras irrelevantes. Podemos importar uma lista de palavras irrelevantes em inglês do NLTK (verifique a documentação para mais idiomas e informações)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2XcKlaMnkD7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_Ci8ROqnkD9"
      },
      "source": [
        "Vamos colocar os dois juntos em uma função para aplicá-lo ao nosso DataFrame mais tarde:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6hrp5XRknkD-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyVsdqMgnkD-"
      },
      "source": [
        "Aqui está o DataFrame original novamente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Azv87UbmnkD-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4TaxJVxnkD_"
      },
      "source": [
        "Agora vamos \"tokenizar\" essas mensagens. Tokenização é apenas o termo usado para descrever o processo de conversão de strings de texto normais em uma lista de tokens (palavras que realmente queremos).\n",
        "\n",
        "Vamos ver um exemplo de saída na coluna:\n",
        "\n",
        "**Observação:**\n",
        "Podemos receber alguns avisos ou erros para símbolos que não consideramos ou que não estavam em Unicode (como um símbolo de libra esterlina)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqtJEF2DnkD_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eSpebO7nkD_"
      },
      "source": [
        "### Continuação da normalização\n",
        "\n",
        "Há muitas maneiras de continuar normalizando este texto. Tal como [Stemming](https://en.wikipedia.org/wiki/Stemming) ou distinguir por [parte do discurso](http://www.nltk.org/book/ch05.html).\n",
        "\n",
        "O NLTK possui muitas ferramentas integradas e ótima documentação sobre muitos desses métodos. Às vezes, eles não funcionam bem para mensagens de texto devido à maneira como muitas pessoas tendem a usar abreviações ou taquigrafias, por exemplo:\n",
        "    \n",
        "    'Nah dawg, IDK! Wut time u headin to da club?'\n",
        "    \n",
        "versus\n",
        "\n",
        "    'No dog, I don't know! What time are you heading to the club?'\n",
        "    \n",
        "Alguns métodos de normalização de texto terão problemas com esse tipo de abreviação e, portanto, deixarei você explorar esses métodos mais avançados através do [NLTK book online](http://www.nltk.org/book/).\n",
        "\n",
        "Por enquanto vamos nos concentrar apenas em usar o que temos para converter nossa lista de palavras em um vetor real que o SciKit-Learn pode usar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOyY1xebnkD_"
      },
      "source": [
        "## Vetorização"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcQjE6QKnkEA"
      },
      "source": [
        "Agora, temos as mensagens como listas de tokens (também conhecidas como [lemmas](http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)) e precisamos converter cada uma dessas mensagens em um vetor com o qual os modelos de algoritmo do SciKit Learn possam trabalhar.\n",
        "\n",
        "Faremos isso em três etapas usando o modelo bag-of-words:\n",
        "\n",
        "1. Contando quantas vezes uma palavra ocorre em cada mensagem (conhecida como frequência de termo)\n",
        "\n",
        "2. Pesar as contagens, para que os tokens frequentes tenham um peso menor (frequência inversa do documento)\n",
        "\n",
        "3. Normalizar os vetores para comprimento unitário, para abstrair do comprimento do texto original (L2 norm)\n",
        "\n",
        "Vamos começar o primeiro passo:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VMQqRqDnkEA"
      },
      "source": [
        "Cada vetor terá tantas dimensões quantas forem as palavras únicas no corpus SMS. Primeiro, usaremos o **CountVectorizer** do SciKit Learn. Este modelo converterá uma coleção de documentos de texto em uma matriz de contagem de tokens.\n",
        "\n",
        "Podemos imaginar isso como uma matriz bidimensional. Onde a dimensão 1 é todo o vocabulário (1 linha por palavra) e a outra dimensão são os documentos reais, neste caso uma coluna por mensagem de texto.\n",
        "\n",
        "Por exemplo:\n",
        "\n",
        "<table border = “1“>\n",
        "<tr>\n",
        "<th></th> <th>Message 1</th> <th>Message 2</th> <th>...</th> <th>Message N</th> \n",
        "</tr>\n",
        "<tr>\n",
        "<td><b>Word 1 Count</b></td><td>0</td><td>1</td><td>...</td><td>0</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><b>Word 2 Count</b></td><td>0</td><td>0</td><td>...</td><td>0</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><b>...</b></td> <td>1</td><td>2</td><td>...</td><td>0</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><b>Word N Count</b></td> <td>0</td><td>1</td><td>...</td><td>1</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "\n",
        "Como há tantas mensagens, podemos esperar muitas contagens com zero para a presença de uma dada palavra em um determinado documento. Por causa disso, o SciKit Learn produzirá uma [Sparse Matrix](https://en.wikipedia.org/wiki/Sparse_matrix)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7fIr9oBnkEA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWhPgG3UnkEA"
      },
      "source": [
        "Existem muitos argumentos e parâmetros que podem ser passados ​​para o CountVectorizer. Neste caso vamos apenas especificar que o **analyzer** seja a nossa função previamente definida:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rgCaQe2nkEA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdmmTlPqnkEA"
      },
      "source": [
        "Vamos pegar uma mensagem de texto e obter sua contagem de palavras como um vetor, usando nosso novo `bow_transformer`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luMjdBednkEB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2afcZfrXnkEB"
      },
      "source": [
        "Agora vamos ver sua representação vetorial:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PagGcm6nkEB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQfuxUiFnkEB"
      },
      "source": [
        "Isso significa que há sete palavras únicas na mensagem número 4 (depois de remover as palavras de parada comuns). Dois deles aparecem duas vezes, o resto apenas uma vez. Vamos verificar e confirmar quais aparecem duas vezes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLvXYmh8nkED"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS422vsJnkED"
      },
      "source": [
        "Agora podemos usar **.transform** em nosso objeto transformado Bag-of-Words (bow) e transformar todo o DataFrame de mensagens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_MHuE5y9nkED"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_e7WIyJnkEE"
      },
      "source": [
        "Após a contagem, a ponderação e a normalização dos termos podem ser feitas com [TF-IDF](http://en.wikipedia.org/wiki/Tf%E2%80%93idf), utilizando scikit-learn `TfidfTransformer`.\n",
        "\n",
        "### O que é TF-IDF?\n",
        "TF-IDF significa *term frequency-inverse document frequency*, e o tf-idf weight é um peso frequentemente usado na recuperação de informações e mineração de texto. Esse peso é uma medida estatística usada para avaliar a importância de uma palavra para um documento em uma coleção ou corpus. A importância aumenta proporcionalmente ao número de vezes que uma palavra aparece no documento, mas é compensada pela frequência da palavra no corpus. Variações do esquema de ponderação tf-idf são frequentemente usadas pelos mecanismos de pesquisa como uma ferramenta central na pontuação e classificação da relevância de um documento dada a consulta do usuário.\n",
        "\n",
        "Uma das funções de classificação mais simples é calculada somando o tf-idf para cada termo de consulta; muitas funções de classificação mais sofisticadas são variantes desse modelo simples.\n",
        "\n",
        "Normalmente, o tf-idf weight é composto por dois termos: o primeiro calcula a normalized Term Frequency (TF), aka. o número de vezes que uma palavra aparece em um documento, dividido pelo número total de palavras naquele documento; o segundo termo é a Frequência Inversa de Documentos (IDF), calculada como o logaritmo do número de documentos no corpus dividido pelo número de documentos onde o termo específico aparece.\n",
        "\n",
        "**TF: Term Frequency**, mede a frequência com que um termo ocorre em um documento. Como cada documento é diferente em tamanho, é possível que um termo apareça muito mais vezes em documentos longos do que em documentos mais curtos. Assim, a frequência do termo é frequentemente dividida pelo comprimento do documento (também conhecido como o número total de termos no documento) como forma de normalização:\n",
        "\n",
        "*TF(t) = (Número de vezes que o termo t aparece em um documento) / (Número total de termos no documento).*\n",
        "\n",
        "**IDF: Inverse Document Frequency**, mede a importância de um termo. Ao calcular TF, todos os termos são considerados igualmente importantes. No entanto, sabe-se que certos termos, como “é”, “de” e “isso”, podem aparecer muitas vezes, mas têm pouca importância. Assim, precisamos pesar os termos frequentes enquanto escalamos os raros, calculando o seguinte:\n",
        "\n",
        "*IDF(t) = log(Número total de documentos / Número de documentos com termo t nele).*\n",
        "\n",
        "Veja abaixo um exemplo simples.\n",
        "\n",
        "**Exemplo:**\n",
        "\n",
        "Considere um documento contendo 100 palavras em que a palavra gato aparece 3 vezes.\n",
        "\n",
        "O termo frequência (ou seja, tf) para gato é então (3/100) = 0,03. Agora, suponha que temos 10 milhões de documentos e a palavra gato aparece em mil deles. Então, a frequência inversa do documento (ou seja, idf) é calculada como log(10.000.000 / 1.000) = 4. Assim, o peso Tf-idf é o produto dessas quantidades: 0,03 * 4 = 0,12."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvE0NpOvnkEE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TB1AJxVynkEE"
      },
      "source": [
        "Vamos em frente e verificar qual é o IDF (inverse document frequency) \n",
        "da palavra `\"u\"` e de palavra `\"university\"`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkOOxavUnkEE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvZoY0XPnkEF"
      },
      "source": [
        "Para transformar todo o corpus bag-of-words em corpus TF-IDF de uma só vez:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oxwl4Hi_nkEF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_1ZDVSznkEF"
      },
      "source": [
        "Há muitas maneiras de pré-processar e vetorizar os dados. Essas etapas envolvem feature engineering e construção de um \"pipeline\". Convido você a verificar a documentação do SciKit Learn sobre como lidar com dados de texto, bem como a ampla coleção de artigos e livros disponíveis sobre o tópico geral de NLP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjEPVOBZnkEF"
      },
      "source": [
        "## Treinando um modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdKis0XRnkEF"
      },
      "source": [
        "Com as mensagens representadas como vetores, podemos finalmente treinar nosso classificador de spam/ham. Agora podemos usar praticamente qualquer tipo de algoritmo de classificação. Por [uma variedade de razões](http://www.inf.ed.ac.uk/teaching/courses/inf2b/learnnotes/inf2b-learn-note07-2up.pdf), o algoritmo classificador Naive Bayes é uma boa escolha."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2AE0p03nkEG"
      },
      "source": [
        "\n",
        "Estaremos usando o scikit-learn aqui, escolhendo o [Naive Bayes](http://en.wikipedia.org/wiki/Naive_Bayes_classifier) classificador para começar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXl1dwjdnkEG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWwzUcRKnkEG"
      },
      "source": [
        "Vamos tentar classificar nossa única mensagem aleatória e verificar como fazemos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrXnV02znkEG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1iWkFUJnkEG"
      },
      "source": [
        "Fantástico! Desenvolvemos um modelo que pode tentar prever a classificação de spam versus ham!\n",
        "\n",
        "## Avaliação do modelo\n",
        "\n",
        "Agora queremos determinar o desempenho geral do nosso modelo em todo o conjunto de dados. Vamos começar obtendo todas as previsões:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8m6N_5cnkEG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpREpgspnkEG"
      },
      "source": [
        "Podemos usar o relatório de classificação integrado do SciKit Learn, que retorna [precision, recall,](https://en.wikipedia.org/wiki/Precision_and_recall) [f1-score](https://en.wikipedia.org/wiki/F1_score), e uma coluna para suporte (ou seja, quantos casos deram suporte a essa classificação). Confira os links para informações mais detalhadas sobre cada uma dessas métricas e a figura abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxyNU0gKnkEH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD15F8MBnkEH"
      },
      "source": [
        "Existem algumas métricas possíveis para avaliar o desempenho do modelo. Qual é o mais importante depende da tarefa e dos efeitos de negócios das decisões baseadas no modelo. Por exemplo, o custo de prever erroneamente \"spam\" como \"ham\" é provavelmente muito menor do que prever erroneamente \"ham\" como \"spam\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KslmTtrCnkEH"
      },
      "source": [
        "Na \"avaliação\" acima, avaliamos a precisão nos mesmos dados que usamos para o treinamento. **Você nunca deve avaliar no mesmo conjunto de dados em que treina!**\n",
        "\n",
        "Tal avaliação não nos diz nada sobre o verdadeiro poder preditivo do nosso modelo. Se simplesmente lembrássemos de cada exemplo durante o treinamento, a precisão nos dados de treinamento seria trivialmente 100%, mesmo que não pudéssemos classificar nenhuma mensagem nova.\n",
        "\n",
        "Uma maneira adequada é dividir os dados em um conjunto de treinamento/teste, em que o modelo só vê os **dados de treinamento** durante o ajuste do modelo e o ajuste dos parâmetros. Os **dados de teste** nunca são usados ​​de forma alguma. Esta é, então, nossa avaliação final sobre os dados de teste é representativa do verdadeiro desempenho preditivo.\n",
        "\n",
        "## Divisão de teste de treino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLWGTiMwnkEH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5W68Iv3nkEH"
      },
      "source": [
        "O tamanho do teste é 20% de todo o conjunto de dados (1.115 mensagens de um total de 5.572), e o treinamento é o restante (4.457 de 5.572). Observe que a divisão padrão teria sido 30/70.\n",
        "\n",
        "## Criando um pipeline de dados\n",
        "\n",
        "Vamos executar nosso modelo novamente e, em seguida, prever fora do conjunto de teste. Usaremos o SciKit Learn [pipeline](http://scikit-learn.org/stable/modules/pipeline.html) para armazenar um pipeline de fluxo de trabalho. Isso nos permitirá configurar todas as transformações que faremos nos dados para uso futuro. Vamos ver um exemplo de como funciona:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ry4_MMd7nkEI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HE549PPnkEI"
      },
      "source": [
        "Agora podemos passar diretamente os dados de texto da mensagem e o pipeline fará nosso pré-processamento para nós! Podemos tratá-lo como uma API de modelo/estimador:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WpB8-bunkEI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwE7MsEbnkEJ"
      },
      "source": [
        "Agora temos um relatório de classificação para nosso modelo em um verdadeiro conjunto de testes!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZbxq-aLnkEJ"
      },
      "source": [
        "## Mais recursos\n",
        "\n",
        "Confira os links abaixo para obter mais informações sobre o Processamento de Linguagem Natural:\n",
        "\n",
        "[NLTK Book Online](http://www.nltk.org/book/)\n",
        "\n",
        "[Kaggle Walkthrough](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words)\n",
        "\n",
        "[SciKit Learn's Tutorial](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.1"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0eSpebO7nkD_",
        "CZbxq-aLnkEJ"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}